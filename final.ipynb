{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc7ba87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      " * Serving Flask app '__main__' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x000001D69B091CD0>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend.py\", line 4650, in <genexpr>\n",
      "    ta.write(time, out) for ta, out in zip(output_ta_t, flat_output))  File \"C:\\Users\\Dell\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 247, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [28/Feb/2022 12:51:05] \"GET /predic1/there%20is%20a%20calcified%20granuloma%20in%20the%20left%20upper%20lobe%20.%20lungs%20othenlvise%20arebelie HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask,jsonify\n",
    "from flask_restful import Resource, Api\n",
    "from flask_cors import CORS\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from flashtext import KeywordProcessor\n",
    "\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "api = Api(app)\n",
    "CORS(app)\n",
    "\n",
    "class status (Resource):\n",
    "    def get(self):\n",
    "        try: \n",
    "            return {'data': 'Api is running'}\n",
    "        except:\n",
    "            return{'data':'an Error occur during fetching Api'}\n",
    "             \n",
    "class predict (Resource):\n",
    "    def get(self, text):\n",
    "        # Global parameters\n",
    "        #root folder\n",
    "        root_folder='.'\n",
    "        #data_folder='.'\n",
    "        data_folder_name='data'\n",
    "        train_filename='raw_dataff.txt'\n",
    "\n",
    "        # Variable for data directory\n",
    "        DATA_PATH = os.path.abspath(os.path.join(root_folder, data_folder_name))\n",
    "        train_filenamepath = os.path.abspath(os.path.join(DATA_PATH, train_filename))\n",
    "\n",
    "        # Both train and test set are in the root data directory\n",
    "        train_path = DATA_PATH\n",
    "        test_path = DATA_PATH\n",
    "\n",
    "        # Parameters for our model\n",
    "        INPUT_COLUMN = 'input'\n",
    "        TARGET_COLUMN = 'target'\n",
    "        TARGET_FOR_INPUT = 'target_for_input'\n",
    "        NUM_SAMPLES = 2500\n",
    "        MAX_VOCAB_SIZE = 2000\n",
    "        EMBEDDING_DIM = 64\n",
    "        HIDDEN_DIM=256 #1024 #512\n",
    "\n",
    "        BATCH_SIZE = 8  # Batch size for training.\n",
    "        EPOCHS = 20  # Number of epochs to train for.\n",
    "\n",
    "        ATTENTION_FUNC='general'\n",
    "\n",
    "# Some function to preprocess the text data, taken from the Neural machine translation with attention tutorial\n",
    "# in Tensorflow\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    ''' Preprocess the input text w applying lowercase, removing accents, \n",
    "    creating a space between a word and the punctuation following it and \n",
    "    replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    Input:\n",
    "        - w: a string, input text\n",
    "    Output:\n",
    "        - a string, the cleaned text\n",
    "    '''\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "    w = w.strip()\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    #w = '<start> ' + w + ' <end>'\n",
    "    \n",
    "    return w\n",
    "# Global parameters\n",
    "#root folder\n",
    "root_folder='.'\n",
    "#data_folder='.'\n",
    "data_folder_name='data'\n",
    "train_filename='raw_dataff.txt'\n",
    "\n",
    "# Variable for data directory\n",
    "DATA_PATH = os.path.abspath(os.path.join(root_folder, data_folder_name))\n",
    "train_filenamepath = os.path.abspath(os.path.join(DATA_PATH, train_filename))\n",
    "\n",
    "# Both train and test set are in the root data directory\n",
    "train_path = DATA_PATH\n",
    "test_path = DATA_PATH\n",
    "\n",
    "# Parameters for our model\n",
    "INPUT_COLUMN = 'input'\n",
    "TARGET_COLUMN = 'target'\n",
    "TARGET_FOR_INPUT = 'target_for_input'\n",
    "NUM_SAMPLES = 2500 #40000\n",
    "MAX_VOCAB_SIZE = 2000\n",
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_DIM=256 #1024 #512\n",
    "\n",
    "BATCH_SIZE = 8  # Batch size for training.\n",
    "EPOCHS = 20  # Number of epochs to train for.\n",
    "\n",
    "ATTENTION_FUNC='general'\n",
    "\n",
    "# Load the dataset: sentence in english, sentence in spanish \n",
    "df=pd.read_csv(train_filenamepath, sep=\"\\t\", header=None, names=[INPUT_COLUMN,TARGET_COLUMN], usecols=[0,1], \n",
    "               nrows=NUM_SAMPLES)\n",
    "# Preprocess the input data\n",
    "input_data=df[INPUT_COLUMN].apply(lambda x : preprocess_sentence(str(x))).tolist()\n",
    "# Preprocess and include the end of sentence token to the target text\n",
    "target_data=df[TARGET_COLUMN].apply(lambda x : preprocess_sentence(str(x))+ ' <eos>').tolist()\n",
    "# Preprocess and include a start of setence token to the input text to the decoder, it is rigth shifted\n",
    "target_input_data=df[TARGET_COLUMN].apply(lambda x : '<sos> '+ preprocess_sentence(str(x))).tolist()\n",
    "\n",
    "\n",
    "\n",
    "df.dropna(subset = [\"input\"], inplace=True)\n",
    "df.dropna(subset = [\"target\"], inplace=True)\n",
    "\n",
    "\n",
    "# Create a tokenizer for the input texts and fit it to them \n",
    "tokenizer_inputs = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='')\n",
    "tokenizer_inputs.fit_on_texts(input_data)\n",
    "# Tokenize and transform input texts to sequence of integers\n",
    "input_sequences = tokenizer_inputs.texts_to_sequences(input_data)\n",
    "# Claculate the max length\n",
    "input_max_len = max(len(s) for s in input_sequences)\n",
    "\n",
    "# Show some example of tokenize sentences, useful to check the tokenization\n",
    "\n",
    "# tokenize the outputs\n",
    "# don't filter out special characters (filters = '')\n",
    "# otherwise <sos> and <eos> won't appear\n",
    "# By default, Keras’ Tokenizer will trim out all the punctuations, which is not what we want. \n",
    "# we can just set filters as blank here.\n",
    "\n",
    "# Create a tokenizer for the output texts and fit it to them \n",
    "tokenizer_outputs = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='')\n",
    "tokenizer_outputs.fit_on_texts(target_data)\n",
    "tokenizer_outputs.fit_on_texts(target_input_data)\n",
    "# Tokenize and transform output texts to sequence of integers\n",
    "target_sequences = tokenizer_outputs.texts_to_sequences(target_data)\n",
    "target_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_input_data)\n",
    "\n",
    "# determine maximum length output sequence\n",
    "target_max_len = max(len(s) for s in target_sequences)\n",
    "\n",
    "# get the word to index mapping for input language\n",
    "word2idx_inputs = tokenizer_inputs.word_index\n",
    "\n",
    "# get the word to index mapping for output language\n",
    "word2idx_outputs = tokenizer_outputs.word_index\n",
    "\n",
    "# store number of output and input words for later\n",
    "# remember to add 1 since indexing starts at 1\n",
    "num_words_output = len(word2idx_outputs) + 1\n",
    "num_words_inputs = len(word2idx_inputs) + 1\n",
    "\n",
    "# map indexes back into real words\n",
    "# so we can view the results\n",
    "idx2word_inputs = {v:k for k, v in word2idx_inputs.items()}\n",
    "idx2word_outputs = {v:k for k, v in word2idx_outputs.items()}\n",
    "\n",
    "\n",
    "# pad the input sequences\n",
    "encoder_inputs = pad_sequences(input_sequences, maxlen=input_max_len, padding='post')\n",
    "# pad the decoder input sequences\n",
    "decoder_inputs = pad_sequences(target_sequences_inputs, maxlen=target_max_len, padding='post')\n",
    "# pad the target output sequences\n",
    "decoder_targets = pad_sequences(target_sequences, maxlen=target_max_len, padding='post')\n",
    "\n",
    "\n",
    "# Define a dataset \n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (encoder_inputs, decoder_inputs, decoder_targets))\n",
    "dataset = dataset.shuffle(len(input_data)).batch(\n",
    "    BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Define the embedding layer\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        # Define the RNN layer, LSTM\n",
    "        self.lstm = tf.keras.layers.LSTM(\n",
    "            hidden_dim, return_sequences=True, return_state=True)\n",
    "\n",
    "    def call(self, input_sequence, states):\n",
    "        # Embed the input\n",
    "        embed = self.embedding(input_sequence)\n",
    "        # Call the LSTM unit\n",
    "        output, state_h, state_c = self.lstm(embed, initial_state=states)\n",
    "\n",
    "        return output, state_h, state_c\n",
    "\n",
    "    def init_states(self, batch_size):\n",
    "        # Return a all 0s initial states\n",
    "        return (tf.zeros([batch_size, self.hidden_dim]),\n",
    "                tf.zeros([batch_size, self.hidden_dim]))\n",
    "    \n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Define the embedding layer\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        # Define the RNN layer, LSTM\n",
    "        self.lstm = tf.keras.layers.LSTM(\n",
    "            hidden_dim, return_sequences=True, return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, input_sequence, state):\n",
    "        # Embed the input\n",
    "        embed = self.embedding(input_sequence)\n",
    "        # Call the LSTM unit\n",
    "        lstm_out, state_h, state_c = self.lstm(embed, state)\n",
    "        # Dense layer to predict output token\n",
    "        logits = self.dense(lstm_out)\n",
    "\n",
    "        return logits, state_h, state_c\n",
    "    \n",
    "#Set the length of the input and output vocabulary\n",
    "num_words_inputs = len(word2idx_inputs) + 1\n",
    "num_words_output = len(word2idx_outputs) + 1\n",
    "#Create the encoder\n",
    "encoder = Encoder(num_words_inputs, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "# Get the initial states\n",
    "initial_state = encoder.init_states(1)\n",
    "# Call the encoder for testing\n",
    "test_encoder_output = encoder(tf.constant(\n",
    "    [[1, 23, 4, 5, 0, 0]]), initial_state)\n",
    "# Create the decoder\n",
    "decoder = Decoder(num_words_output, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "# Get the initial states\n",
    "de_initial_state = test_encoder_output[1:]\n",
    "# Call the decoder for testing\n",
    "test_decoder_output = decoder(tf.constant(\n",
    "    [[1, 3, 5, 7, 9, 0, 0, 0]]), de_initial_state)\n",
    "\n",
    "\n",
    "def loss_func(targets, logits):\n",
    "    crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True)\n",
    "    # Mask padding values, they do not have to compute for loss\n",
    "    mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
    "    mask = tf.cast(mask, dtype=tf.int64)\n",
    "    # Calculate the loss value\n",
    "    loss = crossentropy(targets, logits, sample_weight=mask)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    # y_pred shape is batch_size, seq length, vocab size\n",
    "    # y_true shape is batch_size, seq length\n",
    "    pred_values = K.cast(K.argmax(y_pred, axis=-1), dtype='int32')\n",
    "    correct = K.cast(K.equal(y_true, pred_values), dtype='float32')\n",
    "\n",
    "    # 0 is padding, don't include those\n",
    "    mask = K.cast(K.greater(y_true, 0), dtype='float32')\n",
    "    n_correct = K.sum(mask * correct)\n",
    "    n_total = K.sum(mask)\n",
    "  \n",
    "    return n_correct / n_total\n",
    "\n",
    "# Use the @tf.function decorator to take advance of static graph computation\n",
    "@tf.function\n",
    "def train_step(input_seq, target_seq_in, target_seq_out, en_initial_states, optimizer):\n",
    "    ''' A training step, train a batch of the data and return the loss value reached\n",
    "        Input:\n",
    "        - input_seq: array of integers, shape [batch_size, max_seq_len, embedding dim].\n",
    "            the input sequence\n",
    "        - target_seq_out: array of integers, shape [batch_size, max_seq_len, embedding dim].\n",
    "            the target seq, our target sequence\n",
    "        - target_seq_in: array of integers, shape [batch_size, max_seq_len, embedding dim].\n",
    "            the input sequence to the decoder, we use Teacher Forcing\n",
    "        - en_initial_states: tuple of arrays of shape [batch_size, hidden_dim].\n",
    "            the initial state of the encoder\n",
    "        - optimizer: a tf.keras.optimizers.\n",
    "        Output:\n",
    "        - loss: loss value\n",
    "        \n",
    "    '''\n",
    "    # Network’s computations need to be put under tf.GradientTape() to keep track of gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Get the encoder outputs\n",
    "        en_outputs = encoder(input_seq, en_initial_states)\n",
    "        # Set the encoder and decoder states\n",
    "        en_states = en_outputs[1:]\n",
    "        de_states = en_states\n",
    "        # Get the encoder outputs\n",
    "        de_outputs = decoder(target_seq_in, de_states)\n",
    "        # Take the actual output\n",
    "        logits = de_outputs[0]\n",
    "        # Calculate the loss function\n",
    "        loss = loss_func(target_seq_out, logits)\n",
    "        acc = accuracy_fn(target_seq_out, logits)\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    # Calculate the gradients for the variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    # Apply the gradients and update the optimizer\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return loss, acc\n",
    "\n",
    "# Create the main train function\n",
    "def main_train(encoder, decoder, dataset, n_epochs, batch_size, optimizer, checkpoint, checkpoint_prefix):\n",
    "    \n",
    "    losses = []\n",
    "    accuracies = []\n",
    "\n",
    "    for e in range(n_epochs):\n",
    "        # Get the initial time\n",
    "        start = time.time()\n",
    "        # Get the initial state for the encoder\n",
    "        en_initial_states = encoder.init_states(batch_size)\n",
    "        # For every batch data\n",
    "        for batch, (input_seq, target_seq_in, target_seq_out) in enumerate(dataset.take(-1)):\n",
    "            # Train and get the loss value \n",
    "            loss, accuracy = train_step(input_seq, target_seq_in, target_seq_out, en_initial_states, optimizer)\n",
    "        \n",
    "            if batch % 100 == 0:\n",
    "                # Store the loss and accuracy values\n",
    "                losses.append(loss)\n",
    "                accuracies.append(accuracy)\n",
    "                print('Epoch {} Batch {} Loss {:.4f} Acc:{:.4f}'.format(e + 1, batch, loss.numpy(), accuracy.numpy()))\n",
    "                \n",
    "        # saving (checkpoint) the model every 2 epochs\n",
    "        if (e + 1) % 2 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "        print('Time taken for 1 epoch {:.4f} sec\\n'.format(time.time() - start))\n",
    "        \n",
    "    return losses, accuracies\n",
    "\n",
    "class LuongAttention(tf.keras.Model):\n",
    "    def __init__(self, rnn_size, attention_func):\n",
    "        super(LuongAttention, self).__init__()\n",
    "        self.attention_func = attention_func\n",
    "\n",
    "        if attention_func not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(\n",
    "                'Attention score must be either dot, general or concat.')\n",
    "\n",
    "        if attention_func == 'general':\n",
    "            # General score function\n",
    "            self.wa = tf.keras.layers.Dense(rnn_size)\n",
    "        elif attention_func == 'concat':\n",
    "            # Concat score function\n",
    "            self.wa = tf.keras.layers.Dense(rnn_size, activation='tanh')\n",
    "            self.va = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, decoder_output, encoder_output):\n",
    "        if self.attention_func == 'dot':\n",
    "            # Dot score function: decoder_output (dot) encoder_output\n",
    "            # decoder_output has shape: (batch_size, 1, rnn_size)\n",
    "            # encoder_output has shape: (batch_size, max_len, rnn_size)\n",
    "            # => score has shape: (batch_size, 1, max_len)\n",
    "            score = tf.matmul(decoder_output, encoder_output, transpose_b=True) # (batch_size, 1, max_len)\n",
    "        elif self.attention_func == 'general':\n",
    "            # General score function: decoder_output (dot) (Wa (dot) encoder_output)\n",
    "            # decoder_output has shape: (batch_size, 1, rnn_size)\n",
    "            # encoder_output has shape: (batch_size, max_len, rnn_size)\n",
    "            # => score has shape: (batch_size, 1, max_len)\n",
    "            score = tf.matmul(decoder_output, self.wa(\n",
    "                encoder_output), transpose_b=True) #(batch_size, 1, max_len)\n",
    "        elif self.attention_func == 'concat':\n",
    "            # Concat score function: va (dot) tanh(Wa (dot) concat(decoder_output + encoder_output))\n",
    "            # Decoder output must be broadcasted to encoder output's shape first\n",
    "            decoder_output = tf.tile(\n",
    "                decoder_output, [1, encoder_output.shape[1], 1]) #shape (batch size, max len,hidden_dim)\n",
    "\n",
    "            # Concat => Wa => va\n",
    "            # (batch_size, max_len, 2 * rnn_size) => (batch_size, max_len, rnn_size) => (batch_size, max_len, 1)\n",
    "            score = self.va(\n",
    "                self.wa(tf.concat((decoder_output, encoder_output), axis=-1))) # (batch_size, max len, 1)\n",
    "\n",
    "            # Transpose score vector to have the same shape as other two above\n",
    "            # (batch_size, max_len, 1) => (batch_size, 1, max_len)\n",
    "            score = tf.transpose(score, [0, 2, 1]) #(batch_size, 1, max_len)\n",
    "\n",
    "        # alignment a_t = softmax(score)\n",
    "        alignment = tf.keras.activations.softmax(score, axis=-1) #(batch_size, 1, max_len)\n",
    "        \n",
    "        # context vector c_t is the weighted average sum of encoder output\n",
    "        context = tf.matmul(alignment, encoder_output) # (batch_size, 1, hidden_dim)\n",
    "\n",
    "        return context, alignment\n",
    "    \n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, attention_func):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.attention = LuongAttention(hidden_dim, attention_func)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = tf.keras.layers.LSTM(\n",
    "            hidden_dim, return_sequences=True, return_state=True)\n",
    "        self.wc = tf.keras.layers.Dense(hidden_dim, activation='tanh')\n",
    "        self.ws = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, input_sequence, state, encoder_output):\n",
    "        # Remember that the input to the decoder\n",
    "        # is now a batch of one-word sequences,\n",
    "        # which means that its shape is (batch_size, 1)\n",
    "        embed = self.embedding(input_sequence)\n",
    "\n",
    "        # Therefore, the lstm_out has shape (batch_size, 1, hidden_dim)\n",
    "        lstm_out, state_h, state_c = self.lstm(embed, initial_state=state)\n",
    "\n",
    "        # Use self.attention to compute the context and alignment vectors\n",
    "        # context vector's shape: (batch_size, 1, hidden_dim)\n",
    "        # alignment vector's shape: (batch_size, 1, source_length)\n",
    "        context, alignment = self.attention(lstm_out, encoder_output)\n",
    "\n",
    "        # Combine the context vector and the LSTM output\n",
    "        # Before combined, both have shape of (batch_size, 1, hidden_dim),\n",
    "        # so let's squeeze the axis 1 first\n",
    "        # After combined, it will have shape of (batch_size, 2 * hidden_dim)\n",
    "        lstm_out = tf.concat(\n",
    "            [tf.squeeze(context, 1), tf.squeeze(lstm_out, 1)], 1)\n",
    "\n",
    "        # lstm_out now has shape (batch_size, hidden_dim)\n",
    "        lstm_out = self.wc(lstm_out)\n",
    "\n",
    "        # Finally, it is converted back to vocabulary space: (batch_size, vocab_size)\n",
    "        logits = self.ws(lstm_out)\n",
    "\n",
    "        return logits, state_h, state_c, alignment\n",
    "\n",
    "\n",
    "#Set the length of the input and output vocabulary\n",
    "num_words_inputs = len(word2idx_inputs) + 1\n",
    "num_words_output = len(word2idx_outputs) + 1\n",
    "#Create the encoder\n",
    "encoder = Encoder(num_words_inputs, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "decoder = Decoder(num_words_output, EMBEDDING_DIM, HIDDEN_DIM, ATTENTION_FUNC)\n",
    "\n",
    "# Call the encoder and then the decoder\n",
    "initial_state = encoder.init_states(1)\n",
    "encoder_outputs = encoder(tf.constant([[1]]), initial_state)\n",
    "decoder_outputs = decoder(tf.constant(\n",
    "    [[1]]), encoder_outputs[1:], encoder_outputs[0])\n",
    "\n",
    "@tf.function\n",
    "def train_step(input_seq, target_seq_in, target_seq_out, en_initial_states, optimizer):\n",
    "    ''' A training step, train a batch of the data and return the loss value reached\n",
    "        Input:\n",
    "        - input_seq: array of integers, shape [batch_size, max_seq_len, embedding dim].\n",
    "            the input sequence\n",
    "        - target_seq_out: array of integers, shape [batch_size, max_seq_len, embedding dim].\n",
    "            the target seq, our target sequence\n",
    "        - target_seq_in: array of integers, shape [batch_size, max_seq_len, embedding dim].\n",
    "            the input sequence to the decoder, we use Teacher Forcing\n",
    "        - en_initial_states: tuple of arrays of shape [batch_size, hidden_dim].\n",
    "            the initial state of the encoder\n",
    "        - optimizer: a tf.keras.optimizers.\n",
    "        Output:\n",
    "        - loss: loss value\n",
    "        \n",
    "    '''\n",
    "    loss = 0.\n",
    "    acc = 0.\n",
    "    logits = None\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        en_outputs = encoder(input_seq, en_initial_states)\n",
    "        en_states = en_outputs[1:]\n",
    "        de_state_h, de_state_c = en_states\n",
    "\n",
    "        # We need to create a loop to iterate through the target sequences\n",
    "        for i in range(target_seq_out.shape[1]):\n",
    "            # Input to the decoder must have shape of (batch_size, length)\n",
    "            # so we need to expand one dimension\n",
    "            decoder_in = tf.expand_dims(target_seq_in[:, i], 1)\n",
    "            logit, de_state_h, de_state_c, _ = decoder(\n",
    "                decoder_in, (de_state_h, de_state_c), en_outputs[0])\n",
    "\n",
    "            # The loss is now accumulated through the whole batch\n",
    "            loss += loss_func(target_seq_out[:, i], logit)\n",
    "            # Store the logits to calculate the accuracy\n",
    "            logit = K.expand_dims(logit, axis=1)\n",
    "            if logits is None:\n",
    "                logits = logit\n",
    "            else:\n",
    "                logits = K.concatenate((logits,logit), axis=1)\n",
    "        # Calculate the accuracy for the batch data        \n",
    "        acc = accuracy_fn(target_seq_out, logits)\n",
    "    # Update the parameters and the optimizer\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return loss / target_seq_out.shape[1], acc\n",
    "\n",
    "# Create an Adam optimizer and clips gradients by norm\n",
    "optimizer = tf.keras.optimizers.Adam(clipnorm=5.0)\n",
    "# Create a checkpoint object to save the model\n",
    "checkpoint_dir = './training_ckpt_seq2seq_att'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)\n",
    "\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "def predict_seq2seq_att(input_text, input_max_len, tokenizer_inputs, word2idx_outputs, idx2word_outputs):\n",
    "    if input_text is None:\n",
    "        input_text = input_data[np.random.choice(len(input_data))]\n",
    "    # Tokenize the input text\n",
    "    input_seq = tokenizer_inputs.texts_to_sequences([input_text])\n",
    "    # Pad the sentence\n",
    "    input_seq = pad_sequences(input_seq, maxlen=input_max_len, padding='post')\n",
    "    # Get the encoder initial states\n",
    "    en_initial_states = encoder.init_states(1)\n",
    "    # Get the encoder outputs or hidden states\n",
    "    en_outputs = encoder(tf.constant(input_seq), en_initial_states)\n",
    "    # Set the decoder input to the sos token\n",
    "    de_input = tf.constant([[word2idx_outputs['<sos>']]])\n",
    "    # Set the initial hidden states of the decoder to the hidden states of the encoder\n",
    "    de_state_h, de_state_c = en_outputs[1:]\n",
    "    \n",
    "    out_words = []\n",
    "    alignments = []\n",
    "\n",
    "    while True:\n",
    "        # Get the decoder with attention output\n",
    "        de_output, de_state_h, de_state_c, alignment = decoder(\n",
    "            de_input, (de_state_h, de_state_c), en_outputs[0])\n",
    "        de_input = tf.expand_dims(tf.argmax(de_output, -1), 0)\n",
    "        # Detokenize the output\n",
    "        out_words.append(idx2word_outputs[de_input.numpy()[0][0]])\n",
    "        # Save the aligment matrix\n",
    "        alignments.append(alignment.numpy())\n",
    "\n",
    "        if out_words[-1] == '<eos>' or len(out_words) >= 20:\n",
    "            break\n",
    "    # Join the output words\n",
    "    return np.array(alignments), input_text.split(' '), out_words\n",
    "\n",
    "\n",
    "def translation(inputtext):\n",
    "    keydata=pd.read_csv('dict.csv')\n",
    "    keyword_processor = KeywordProcessor()\n",
    "\n",
    "    inputQuery1 = inputtext\n",
    "    # add keywords\n",
    "    keyword_names = keydata[\"WORDS\"]\n",
    "    clean_names = keydata[\"MEANING\"]\n",
    "    for keyword_name, clean_name in zip(keyword_names, clean_names):\n",
    "        keyword_processor.add_keyword(keyword_name, clean_name)\n",
    "    text=(inputQuery1) \n",
    "    keywords_found = keyword_processor.extract_keywords(text)\n",
    "    new_sentence = keyword_processor.replace_keywords(text)\n",
    "    \n",
    "    return new_sentence \n",
    "\n",
    "class predict1 (Resource):\n",
    "    def get(self, text): \n",
    "        n_predictions=1\n",
    "        #test_sents = input_data[1:(1+n_predictions)]\n",
    "        input_data = text\n",
    "        test_sents = input_data\n",
    "\n",
    "        # Create the figure to plot in\n",
    "        #fig = plt.figure(figsize=(10, 20))\n",
    "        for i, test_sent in enumerate(test_sents):\n",
    "            # Call the predict function to get the translation\n",
    "            alignments, source, finalprediction = predict_seq2seq_att(test_sent, input_max_len, tokenizer_inputs, \n",
    "                                                             word2idx_outputs, idx2word_outputs)\n",
    "            attention = np.squeeze(alignments, (1, 2))\n",
    "\n",
    "        predict = finalprediction\n",
    "        sep = ' '\n",
    "        res = sep.join(prediction)\n",
    "        responce1 = res.replace(\" <eos>\", \"\")\n",
    "        return jsonify({'data': responce1, 'tranlation':translation(responce1)})\n",
    "\n",
    "api.add_resource(status,'/')\n",
    "api.add_resource(predict,'/predic/<string:text>')\n",
    "api.add_resource(predict1,'/predic1/<string:text>')\n",
    "\n",
    "if __name__=='__main__':\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dd307e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
